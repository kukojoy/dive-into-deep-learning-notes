{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a07ea5",
   "metadata": {},
   "source": [
    "# 3.4 softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf1d6b",
   "metadata": {},
   "source": [
    "“回归”可以用于预测目标数值上的“多少”，而很多时候，我们也对“分类”问题感兴趣，即预测“是哪一类”。\n",
    "\n",
    "机器学习的实践者把“分类”分为两种类型：\n",
    "\n",
    "1. 硬分类：我们想知道样本的“硬性”类别。\n",
    "2. 软分类：我们想知道样本属于某一个类别的概率。\n",
    "\n",
    "事实上，这两者的界限很模糊，因为即使我们只关心硬性类别，我们仍然会使用软性类别的预测模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704fd4c",
   "metadata": {},
   "source": [
    "## 3.4.1 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e878b",
   "metadata": {},
   "source": [
    "我们从一个图像分类问题开始。\n",
    "\n",
    "首先，假设输入是一个2 x 2像素的灰度图像，我们可以标量表示每个像素值，则每张图片对应了4个特征$ x_{1}, x_{2}, x_{3}, x_{4} $。此外，假设每张图片属于类别猫、鸡、狗的一个。\n",
    "\n",
    "接下来，我们考虑如何表示标签，我们有两个方法：\n",
    "\n",
    "1. 最直接的，选择$ y \\in \\{1, 2, 3\\} $，分别对应猫、鸡、狗。若类别间具有一定的自然顺序，例如预测{儿童, 少年, 成人, 老年人}，那么用这样的格式是有意义的。\n",
    "\n",
    "2. 一般的分类问题，类别之间是不存在自然关系的。这时可以使用独热编码（one-hot ecoding），令标签为一个向量，维度与类别总数一致，对应类别的分量置为1，其余为0。在我们的例子中，可以用(1, 0, 0)表示猫，(0, 1, 0）表示鸡，(0, 0, 1)表示狗：\n",
    "\n",
    "$$ y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6f0e4",
   "metadata": {},
   "source": [
    "## 3.4.2 网络架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102340d7",
   "metadata": {},
   "source": [
    "为解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function），每个输出对应于它自己的仿射函数。在这个例子中，我们有4个特征和3个可能的输出类别，因此，我们需要12个标量来表示权重，3个标量来表示偏置：\n",
    "\n",
    "$$ o_{1} = x_{1}w_{11} + x_{2}w_{12} + x_{3}w_{13} + x_{4}w_{14} + b_{1} $$\n",
    "$$ o_{2} = x_{1}w_{21} + x_{2}w_{22} + x_{3}w_{23} + x_{4}w_{24} + b_{2} $$\n",
    "$$ o_{3} = x_{1}w_{31} + x_{2}w_{32} + x_{3}w_{33} + x_{4}w_{34} + b_{3} $$\n",
    "\n",
    "为了更适应数学和编码，我们仍使用向量化计算，将模型表示为$ \\boldsymbol{o} = \\boldsymbol{W} \\boldsymbol{x} + b $。我们把权重放到一个3 x 4的矩阵中。对于给定样本的特征$ \\boldsymbol{x} $，我们可以通过该运算得到输出$ \\boldsymbol{o} $，对应一个长度与类别数相同的软标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05f73f",
   "metadata": {},
   "source": [
    "## 3.4.3 全连接层的参数开销"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3bcbb",
   "metadata": {},
   "source": [
    "顾名思义，全连接层是“完全”连接的，即后一层的每一个输入与前一层的每一个输出是相关联的，可能有很多的参数。具体来说，对于任何具有$ d $个输入和$ q $个输出的全连接层，参数开销为$ O(dq) $，这个数字可能是非常大的。幸运的是，这个成本可以减少到$ O(dq/n) $，其中超参数$ n $可以灵活指定，以在实际应用中在参数节省和模型有效性之间进行权衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99437770",
   "metadata": {},
   "source": [
    "## 3.4.4 softmax运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095f28a",
   "metadata": {},
   "source": [
    "现在，我们将优化参数以最大化观测数据的概率。为了得到预测结果，我们可以选择与预测标签的最大分量对应的类别：\n",
    "\n",
    "我们希望模型的输出$ \\hat{y}_{j} $可以视作属于类$ j $的概率，然后选择具有最大输出值的类别，即$ \\underset{j}{\\arg \\max} y_{j} $作为我们的预测。然而，未规范化的预测$ o $不能直接视作概率输出$ y $，因为其不符合概率论的非负性和规范性。\n",
    "\n",
    "softmax函数能够将$ o $中的每一个分量映射到区间$ [0, 1] $中，并使其总和为1，且具有可导的性质：\n",
    "\n",
    "$$ \\hat{\\boldsymbol{y}} = softmax(\\boldsymbol{o}), \\hat{\\boldsymbol{y}_{j}} = \\frac{exp(o_{j})}{\\sum_{k}exp(o_{k})} $$ \n",
    "\n",
    "softmax还具有单调性，因此，在预测时，我们仍然可以用下式选择最可能的类别：\n",
    "\n",
    "$$ \\underset{j}{\\arg \\max} y_{j} = \\underset{j}{\\arg \\max} o_{j} $$\n",
    "\n",
    "尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。因此，softmax回归是个线性模型（linear model）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48005a",
   "metadata": {},
   "source": [
    "## 3.4.5 小批量样本的向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e58929",
   "metadata": {},
   "source": [
    "假设特征数为$ d $，批量大小为$ n $，类别数为$ q $，则小批量样本$ \\boldsymbol{X} \\in \\mathbb{R}^{n \\times d} $，权重为$ \\boldsymbol{W} \\in \\mathbb{R}^{d \\times q} $，偏置为$ \\boldsymbol{b} \\in \\mathbb{R}^{1 \\times q} $。则：\n",
    "\n",
    "$$ \\boldsymbol{O} = \\boldsymbol{W} \\boldsymbol{X} + \\boldsymbol{b} $$\n",
    "$$ \\hat{\\boldsymbol{Y}} = softmax(\\boldsymbol{O}) $$\n",
    "\n",
    "$ \\boldsymbol{X} $的行数代表了样本数，而softmax可以按行执行，先对所有项进行幂运算，再进行标准化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7527152",
   "metadata": {},
   "source": [
    "## 3.4.6 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545709e",
   "metadata": {},
   "source": [
    "### 1. 对数似然"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24e525",
   "metadata": {},
   "source": [
    "softmax函数给出了一个向量$ \\hat{\\boldsymbol{y}} $，可以视为是给定任意输入的每个类的条件概率。例如，$ \\hat{y}_{1} = P(j = 猫|\\boldsymbol{x}) $。\n",
    "\n",
    "假设整个数据集$ \\boldsymbol{X}, \\boldsymbol{Y} $具有$ n $个样本，其中索引$ i $的样本由特征向量$ \\boldsymbol{x}^{(i)} 和 \\boldsymbol{y}^{(i)} $组成。我们可以将估计值与实际值进行比较：\n",
    "\n",
    "$$ P(\\boldsymbol{Y} | \\boldsymbol{X}) = \\prod_{i = 1}^{n} P(\\boldsymbol{y}^{(i)} | \\boldsymbol{x}^{(i)}) $$\n",
    "\n",
    "根据极大似然估计，可以最小化负对数似然函数：\n",
    "\n",
    "$$ -logP(\\boldsymbol{Y} | \\boldsymbol{X}) = \\sum_{i = 1}^{n} -logP(\\boldsymbol{y}^{(i)} | \\boldsymbol{x}^{(i)}) = \\sum_{i = 1}^{n} l(\\boldsymbol{y}^{(i)}, \\hat{\\boldsymbol{y}}^{(i)})$$\n",
    "\n",
    "其中，损失函数\n",
    "\n",
    "$$ l(\\boldsymbol{y}^{(i)}, \\hat{\\boldsymbol{y}}^{(i)}) = -\\sum_{j = 1}^{q}y_{j}log{\\hat{y_{j}}} $$\n",
    "\n",
    "该损失函数称为交叉熵损失（cross-entropy loss）。因为$ \\boldsymbol{y}^{(i)} $是一个长度为$ q $的独热编码向量，所以除一个项以外的其他项都消失了。由于所有$ \\hat{y}_{j} $都是预测的概率，它们的对数永远不会大于0。因此，如果正确地预测实际标签，即实际标签$ P(\\boldsymbol{y} | \\boldsymbol{x}) = 1 $，则损失函数不能进一步最小化。但是，这往往是不可能的，因为数据集中可能存在标签噪声，或输入特征没有足够的信息来完美地对每个样本分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ce455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
