{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b8c6f3",
   "metadata": {},
   "source": [
    "# 3.3 线性回归的简单实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ebfea",
   "metadata": {},
   "source": [
    "在上一节中，除了张量和反向传播，训练所需要的其他工具（数据迭代器、模型、损失函数、优化器）我们都自己动手实践了一遍。实际上，成熟的深度学习框架已经将这些collections实现好了。本节将介绍如何使用pytorch简洁地实现上一节的线性回归模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059fadd",
   "metadata": {},
   "source": [
    "## 3.3.1 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e0d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d5025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor(4.2)\n",
    "\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5f7b1",
   "metadata": {},
   "source": [
    "## 3.3.2 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa016be",
   "metadata": {},
   "source": [
    "在这里，我们直接调用框架中现有的API来读取数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91615768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"pytorch数据迭代器\"\"\"\n",
    "    \n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    \n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b955dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cb6ba",
   "metadata": {},
   "source": [
    "对data_iter使用内置函数iter()，可以返回一个可迭代的对象，并使用内置函数next()获取其第一个项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43c1689b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.4907, -0.7659],\n",
       "         [ 1.9876,  0.3372],\n",
       "         [-0.7485,  0.1689],\n",
       "         [-0.1056, -0.6278]]),\n",
       " tensor([[9.7773],\n",
       "         [7.0311],\n",
       "         [2.1121],\n",
       "         [6.1386]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10933516",
   "metadata": {},
   "source": [
    "## 3.3.3 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e199ea",
   "metadata": {},
   "source": [
    "对于标准深度学习模型的构建，可以用框架预定义好的模块，这使得我们只需要关注使用哪些模块来构造模型，而不必关注模块的具体实现。\n",
    "\n",
    "首先，我们定义一个Sequential类的实例net，Sequential类可以将多个模块级联在一起。当给定输入数据时，该实例可以将数据传入第一层，并将第一层的输出作为第二层的输入，以此类推。\n",
    "\n",
    "即使模型只有一层（一个模块），我们也会使用Sequential以符合“标准的流水线”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b1d2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e545ef5",
   "metadata": {},
   "source": [
    "## 3.3.4 初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5573b",
   "metadata": {},
   "source": [
    "在使用net之前，还需要初始化模型参数。深度学习框架通常有预定义的方法来初始化参数。在这里，我们指定每个权重参数应该从均值为0、标准差为1的正态分布中随机抽样，偏置参数初始化为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f91d407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net[0]表示Sequential类实例net中的第一层\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf7895",
   "metadata": {},
   "source": [
    "## 3.3.5 定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b647dcd",
   "metadata": {},
   "source": [
    "这里使用均方误差MSELoss类，默认返回所有样本的平均损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a6a37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62da9a4",
   "metadata": {},
   "source": [
    "## 3.3.6 定义优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbd0ba",
   "metadata": {},
   "source": [
    "Pytorch在optim模块中实现了许多优化算法。当我们创建一个SGD实例时，要指定优化的参数（通过net.parameters()）以及所需的超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e72ae77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a0d10",
   "metadata": {},
   "source": [
    "## 3.3.7 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124b934",
   "metadata": {},
   "source": [
    "在上面，我们使用Pytorch框架简洁地实现了训练所需的基本组件，下面的训练过程和我们从零开始实现时的非常相似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0a67c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0041, -0.2703]], requires_grad=True), Parameter containing:\n",
      "tensor([0.], requires_grad=True)]\n",
      "epoch 1, loss 4.215328\n",
      "epoch 2, loss 0.567284\n",
      "epoch 3, loss 0.077122\n",
      "epoch 4, loss 0.010640\n",
      "epoch 5, loss 0.001560\n",
      "epoch 6, loss 0.000307\n",
      "epoch 7, loss 0.000133\n",
      "epoch 8, loss 0.000108\n",
      "epoch 9, loss 0.000105\n",
      "epoch 10, loss 0.000104\n"
     ]
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 1)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "# 查看参数初始化情况\n",
    "print(list(net.parameters()))\n",
    "      \n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a92cc",
   "metadata": {},
   "source": [
    "比较训练得到的参数与真实参数的差异："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6aca74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Loss: tensor([-0.0001, -0.0003])\n",
      "b Loss: tensor(-0.0002)\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "b = net[0].bias.data\n",
    "\n",
    "print('w Loss:', true_w - w.reshape(true_w.shape))\n",
    "print('b Loss:', true_b - b.reshape(true_b.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
