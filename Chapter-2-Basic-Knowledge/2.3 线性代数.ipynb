{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab747717",
   "metadata": {},
   "source": [
    "# 2.3 线性代数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b9986",
   "metadata": {},
   "source": [
    "## 2.3.1 标量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b3977",
   "metadata": {},
   "source": [
    "单个数值被称为标量（scalar），如5、9、32。标量由只有一个元素的张量表示，也称为0维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d32ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(1.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67162626",
   "metadata": {},
   "source": [
    "## 2.3.2 向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd9749",
   "metadata": {},
   "source": [
    "向量（vector）也称为1维张量，是一个由一个或多个标量组成的列表，这些标量值被称为向量的元素（element），或分量（component）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978359be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05176a3d",
   "metadata": {},
   "source": [
    "一般认为列向量是向量的默认方向，我们可以用索引来引用向量中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2748cbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019178f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e597bf3",
   "metadata": {},
   "source": [
    "向量的长度通常称为向量的维度（dimension），可以用python内置的len()或张量的shape属性获取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63306421",
   "metadata": {},
   "source": [
    "注意：维度这个词在不同的场景下有不同的含义，一般认为“向量的维度”指向量的长度，“张量的维度”指张量所具有的轴数，例如我们称x是一个一维张量、四维向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4599c6d",
   "metadata": {},
   "source": [
    "## 2.3.3 矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced0c3e",
   "metadata": {},
   "source": [
    "矩阵（matrix）也称为2维张量，由m行n列的标量元素组成，其中的标量元素也可以用索引获取。当m == n时，它被称为方阵（square matrix）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2a5442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19]]),\n",
       " tensor([[1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A, A[0:1, 1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd69c1b6",
   "metadata": {},
   "source": [
    "交换矩阵的行和列的结果称为矩阵的转置（transpose），m行n列的矩阵转置后变为一个n行m列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4382f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835f4ef",
   "metadata": {},
   "source": [
    "对称矩阵（symmetric matrix）是方阵的一种特殊类型，它的转置等于它本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d484232a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [2, 0, 4],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B, B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988147e1",
   "metadata": {},
   "source": [
    "矩阵是非常有用的数据结构，能组织、表示各种模式的数据，例如在一个数据集中，用每一行表示一个样本，每一列表示一个特征，那么第m行第n列的标量元素就代表了样本m的第n个特征的特征值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49943389",
   "metadata": {},
   "source": [
    "## 2.3.4 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e82548",
   "metadata": {},
   "source": [
    "张量（tensor）是矩阵的推广，是更高维度的数据结构。例如，我们可以用一个三维的张量表示一张用RGB空间表示的彩色图像，三个维度的含义分别为：宽度、高度，以及用于表示颜色（红、绿、蓝）的通道数（channel），张量中标量元素的含义为像素值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962dcd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431dc4c1",
   "metadata": {},
   "source": [
    "## 2.3.5 张量算法的基本性质"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f42e4",
   "metadata": {},
   "source": [
    "张量的按元素一元运算不会改变其形状。给定任意两个具有相同形状的张量，任何按元素二元运算的结果也具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af97978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " torch.Size([5, 4]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()\n",
    "A, A.shape, A + B, (A + B).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe4f90",
   "metadata": {},
   "source": [
    "两个矩阵的按元素乘法称为哈达玛积（Hadamard product）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b355ed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516715ef",
   "metadata": {},
   "source": [
    "将张量加上或乘以一个标量不会改变其形状，通过广播机制可以将标量扩展为张量，再进行按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e7cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.],\n",
       "         [18., 19., 20., 21.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "a + A, a * A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c012e01",
   "metadata": {},
   "source": [
    "## 2.3.6 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823f330",
   "metadata": {},
   "source": [
    "求和是非常有用的张量运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307c57ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49075e1b",
   "metadata": {},
   "source": [
    "默认情况下，求和函数会沿着所有轴降低张量的维度，返回一个标量。我们可以指定沿哪个轴通过求和来降低维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4549fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([40., 45., 50., 55.]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "A_sum_axis0 = A.sum(axis=0) # 对轴0求和\n",
    "A, A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d4f40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([ 6., 22., 38., 54., 70.]),\n",
       " torch.Size([5]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1) # 对轴1求和\n",
    "A, A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4c4469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) # 对轴0, 1求和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fce4b",
   "metadata": {},
   "source": [
    "另一个与求和相关的量是平均值（mean or average），计算平均值也可以通过指定或不指定轴来降维。此外，计算最大值也能降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d656963e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9c5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637431d",
   "metadata": {},
   "source": [
    "有时，我们不希望直接消除某个轴，而是让这个轴的长度变为1，即张量的维度不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "296b30ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A # m行n列变为m行1列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b4d73",
   "metadata": {},
   "source": [
    "由于sum_A的维度不变，可以通过广播将A除以sum_A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308733c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220565fa",
   "metadata": {},
   "source": [
    "使用cumsum()方法也不会降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73a27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5770c7",
   "metadata": {},
   "source": [
    "## 2.3.7 点积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539e343",
   "metadata": {},
   "source": [
    "点积就是两个张量哈达玛积的求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0baade3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.), tensor(6.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y), torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1a9c0",
   "metadata": {},
   "source": [
    "点积有非常广泛的应用，例如可以用来计算加权和，当权重和为1时，可以表示加权平均。此外，两个向量的方向向量的点积表示它们夹角的余弦值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f3a02",
   "metadata": {},
   "source": [
    "## 2.3.8 矩阵-向量积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90e762",
   "metadata": {},
   "source": [
    "一个矩阵（A）和一个向量（x）的矩阵-向量积（matrix-vector product）是一个向量（b）。其中，满足b的第i个元素值为A的第i个行向量与x向量的点积。显然，应该满足A的列数与x的长度相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1434658f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4]),\n",
       " tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, x, A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a33dc2",
   "metadata": {},
   "source": [
    "因此，我们可以将矩阵A看作是向量x到向量b的转换，在后续的章节会讲到，这种转换在计算神经网络中非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22198be5",
   "metadata": {},
   "source": [
    "## 2.3.9 矩阵-矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b22cb9",
   "metadata": {},
   "source": [
    "两个矩阵（A，B）的矩阵-矩阵乘法（可简称为矩阵乘法）的结果是一个矩阵（X），其中，X满足X\\[i][j]的元素值为A的第i个行向量与B的第j个列向量的点积。显然，应该满足A的列数与B的行数相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd2801f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b7981",
   "metadata": {},
   "source": [
    "## 2.3.10 范数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d317eac",
   "metadata": {},
   "source": [
    "线性代数中最有用的一些运算符是范数（norm）。非正式的说，向量的范数运算将向量映射到标量，这个标量可以表示该向量的大小。\n",
    "向量范数的三个性质：\n",
    "1. $ f(\\alpha x) = | \\alpha | f(x) $\n",
    "2. $ f(x + y) \\leqslant f(x) + f(y) $\n",
    "3. $ f(x) \\leqslant 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a43952",
   "metadata": {},
   "source": [
    "范数很像距离的度量，例如L2范数就是我们最常用的欧几里得距离：\n",
    "$$ || x ||_2 = \\sqrt{\\sum_{i = 1}^n{ x_i^2 }} $$\n",
    "其中，L2范数常常省略下标，也就是说$ || x ||_2 $等同于$ || x || $，在代码中，我们可以这样计算向量的L2范数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8102248e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ef624",
   "metadata": {},
   "source": [
    "在深度学习中，经常使用L2范数的平方，也经常遇到L1范数，它为向量元素的绝对值之和：\n",
    "$$ || x ||_1 = \\sum_{i=1}^n | x_i |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db3086",
   "metadata": {},
   "source": [
    "与L2范数相比，L1范数受异常值的影响较小。为了计算L1范数，我们将绝对值函数与按元素求和组合起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d819f07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254118a2",
   "metadata": {},
   "source": [
    "L2范数和L1范数都是更一般的Lp范数的特例：\n",
    "$$ || x ||_p = (\\sum_{i = 1}^n {x_i}^p)^{1/p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd7495",
   "metadata": {},
   "source": [
    "类似于向量的L2范数，矩阵的弗罗贝尼乌斯范数（Frobenius norm）是矩阵元素平方和的平方根：\n",
    "$$ || X ||_F = \\sqrt{\\sum_{i = 1}^m\\sum_{j = 1}^n x_{ij}^2} $$\n",
    "它具有向量范数的所有性质，就像是矩阵形向量的L2范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdace777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones(4, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef860c",
   "metadata": {},
   "source": [
    "**范数有什么用？**\n",
    "\n",
    "范数能够衡量向量/矩阵之间的距离（差距、差异度），有时我们希望将差异度最小化，如预测值和真实标签的差异度，有时我们希望将差异度最大化，如对应了不同标签的特征向量之间的差异度。总之，除了数据，目标或许是深度学习算法最重要的组成部分，而目标通常被表达为范数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "facenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
